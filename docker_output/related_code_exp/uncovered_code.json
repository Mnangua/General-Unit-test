[
  {
    "file_path": "expectation/conformal/adaptivethreshold.py",
    "code": "class AdaptiveThresholdHandler:\n    \"\"\"\n    Handles adaptive thresholding for conformal e-testing by dynamically adjusting the threshold to control the false alarm rate.\n    \"\"\"\n    def __init__(self, target_false_alarm_rate: float = 0.05, learning_rate: float = 0.01, min_threshold: float = 1.0, max_threshold: float = 50.0):\n        self.target_rate = target_false_alarm_rate\n        self.learning_rate = learning_rate\n        self.threshold = 10.0  # Start with a higher initial threshold for sensitivity\n        self.alarm_history = []  # A list of booleans to track if alarms were false or true\n        self.min_threshold = min_threshold  # Ensure that the threshold doesn't drop below this level\n        self.max_threshold = max_threshold  # Ensure that the threshold doesn't exceed this level\n        self.cumulative_error = 0.0  # Track cumulative error for smoother adaptation\n        \n    def update(self, current_rate: float) -> float:\n        # Calculate the difference between the observed rate and the target rate\n        error = current_rate - self.target_rate\n        \n        # Apply a cumulative weighted average to smooth the adaptation\n        self.cumulative_error = 0.9 * self.cumulative_error + 0.1 * error\n        \n        # Apply a non-linear adjustment factor to make adaptation more responsive to large deviations\n        if self.cumulative_error > 0:\n            adjustment_factor = 1 + (self.learning_rate * abs(self.cumulative_error))  # Increase more conservatively if below target\n        else:\n            adjustment_factor = 1 / (1 + (self.learning_rate * abs(self.cumulative_error)))  # Decrease more aggressively if above target\n        \n        # Update the threshold based on the adjustment factor\n        self.threshold *= adjustment_factor\n        \n        # Ensure the threshold does not fall below the minimum value or exceed the maximum value\n        self.threshold = max(self.min_threshold, min(self.threshold, self.max_threshold))\n        \n        return self.threshold\n    \n    def record_alarm(self, is_false_alarm: bool):\n        self.alarm_history.append(is_false_alarm)\n        \n    def get_current_rate(self) -> float:\n        if not self.alarm_history:\n            return 0.0\n        return sum(self.alarm_history) / len(self.alarm_history)\n    \n    def reset(self):\n        self.alarm_history = []\n        self.threshold = 10.0  # Reset to a reasonable initial value\n        self.cumulative_error = 0.0  # Reset cumulative error\n",
    "uncovered_lines": [
      40
    ]
  },
  {
    "file_path": "expectation/conformal/conformal.py",
    "code": "\"\"\"\nThis is an experimental module of the library trying to implement the conformal prediction framework but for e-values.\nThe module is not yet complete and is still under heavy development so use it with caution as no unit tests exist yet.\n\nConformal e-testing (2024) - Vladimir Vovk, Ilia Nouretdinov, and Alex Gammerman\nhttps://www.alrw.net/articles/29.pdf\n\n\"\"\"\n\n\nfrom typing import Optional, List, Tuple, Callable, Union\nimport numpy as np\nfrom numpy.typing import ArrayLike\n\nfrom expectation.modules.martingales import (\n    BetaBinomialMixture, OneSidedNormalMixture, \n    TwoSidedNormalMixture, GammaExponentialMixture\n)\n\nclass ConformalEValue:\n    \"\"\"\n    Implementation of conformal e-values using proper nonconformity e-measures.\n    Fixed to properly handle scaling and normalization.\n    \"\"\"\n    def __init__(self, \n                 nonconformity_type: str = \"normal\",\n                 is_one_sided: bool = True,\n                 v_opt: float = 1.0,\n                 alpha_opt: float = 0.05,\n                 allow_infinite: bool = False):\n        self.allow_infinite = allow_infinite\n        self.v_opt = v_opt\n        self.alpha_opt = alpha_opt\n        \n        # Initialize mixture martingale\n        if nonconformity_type == \"normal\":\n            self.mixture = (OneSidedNormalMixture if is_one_sided \n                          else TwoSidedNormalMixture)(v_opt, alpha_opt)\n        else:\n            raise ValueError(f\"Unsupported nonconformity type: {nonconformity_type}\")\n            \n        self.reset()\n    \n    def reset(self):\n        self._data: List[float] = []\n        self._running_mean = 0.0\n        self._running_var = 1.0\n        self._n_samples = 0\n        \n    def _update_statistics(self, new_data: np.ndarray):\n        for x in new_data:\n            self._n_samples += 1\n            delta = x - self._running_mean\n            self._running_mean += delta / self._n_samples\n            if self._n_samples > 1:\n                delta2 = x - self._running_mean\n                self._running_var = ((self._n_samples - 2) * self._running_var + \n                                   delta * delta2) / (self._n_samples - 1)\n    \n    def compute_nonconformity_score(self, data: np.ndarray) -> float:\n        batch_size = len(data)\n        \n        if self._n_samples == 0:\n            # First batch\n            s = np.sqrt(batch_size) * np.mean(data)\n            v = self.v_opt\n        else:\n            # Compute standardized difference\n            batch_mean = np.mean(data)\n            s = np.sqrt(batch_size) * (batch_mean - self._running_mean) \n            s /= np.sqrt(self._running_var + 1e-8)  # Add small constant for stability\n            v = self.v_opt * (1 + 1/np.sqrt(self._n_samples))\n        \n        # Compute e-score using mixture\n        log_e_score = self.mixture.log_superMG(s, v)\n        e_score = np.exp(log_e_score)\n        \n        if not self.allow_infinite and np.isinf(e_score):\n            raise ValueError(\"Infinite e-value detected and not allowed\")\n            \n        return e_score\n        \n    def update(self, new_data: ArrayLike) -> float:\n        new_data = np.asarray(new_data)\n        \n        # Compute e-value before updating statistics\n        e_value = self.compute_nonconformity_score(new_data)\n        \n        # Update running statistics\n        self._update_statistics(new_data)\n        \n        return e_value\n    \n    @property\n    def n_samples(self) -> int:\n        \"\"\"\n        \n        Number of samples processed.\n        \n        \"\"\"\n        return self._n_samples\n    \n\nclass ConformalEPseudomartingale:\n    \"\"\"\n    Implementation of conformal e-pseudomartingales as described in Section 3 of the paper.\n    Tracks the product of conformal e-values and implements compound betting strategies.\n    \"\"\"\n    def __init__(self, \n                 initial_capital: float = 1.0,\n                 allow_infinite: bool = False):\n        self.initial_capital = initial_capital\n        self.allow_infinite = allow_infinite\n        self.reset()\n        \n    def reset(self):\n        self._capital = self.initial_capital  # Current capital Sₙ\n        self._e_values: List[float] = []  # History of e-values\n        self._capital_history: List[float] = [self.initial_capital]  # History of Sₙ\n        self._max_capital = self.initial_capital  # Running maximum S*_∞\n        \n    def update(self, e_value: float) -> Tuple[float, float]:\n\n        if not self.allow_infinite and np.isinf(e_value):\n            raise ValueError(\"Infinite e-value detected and not allowed\")\n            \n        # Update capital by multiplying with e-value\n        self._capital *= e_value\n        \n        # Update histories\n        self._e_values.append(e_value)\n        self._capital_history.append(self._capital)\n        \n        # Update running maximum\n        self._max_capital = max(self._max_capital, self._capital)\n        \n        return self._capital, self._max_capital\n    \n    def compound_bet(self, e_values: ArrayLike) -> float:\n        e_values = np.asarray(e_values)\n        return float(self.initial_capital * np.prod(e_values))\n    \n    @property\n    def capital(self) -> float:\n        return self._capital\n    \n    @property\n    def max_capital(self) -> float:\n        return self._max_capital\n    \n    @property\n    def n_steps(self) -> int:\n        return len(self._e_values)\n    \n    def get_history(self) -> Tuple[np.ndarray, np.ndarray]:\n        return (np.array(self._e_values), \n                np.array(self._capital_history))\n    \n    def test_threshold(self, threshold: float, use_max: bool = True) -> bool:\n        test_value = self._max_capital if use_max else self._capital\n        return test_value >= threshold\n\nclass TruncatedEPseudomartingale(ConformalEPseudomartingale):\n    def __init__(self,\n                 initial_capital: float = 1.0,\n                 min_capital: float = 1e-10,\n                 allow_infinite: bool = False):\n        super().__init__(initial_capital, allow_infinite)\n        self.min_capital = min_capital\n        \n    def update(self, e_value: float) -> Tuple[float, float]:\n        capital, max_cap = super().update(e_value)\n        \n        # Apply truncation\n        if capital < self.min_capital:\n            self._capital = self.min_capital\n            self._capital_history[-1] = self.min_capital\n            \n        return self._capital, self._max_capital",
    "uncovered_lines": [
      69,
      70,
      71,
      72,
      79,
      101,
      125,
      140,
      141,
      145,
      149,
      153,
      156,
      160,
      161
    ]
  },
  {
    "file_path": "expectation/conformal/cusum.py",
    "code": "\"\"\"\nThis is an experimental module of the library trying to implement the conformal prediction framework but for e-values.\nThe module is not yet complete and is still under heavy development so use it with caution as no unit tests exist yet.\n\nConformal e-testing (2024) - Vladimir Vovk, Ilia Nouretdinov, and Alex Gammerman\nhttps://www.alrw.net/articles/29.pdf\n\n\"\"\"\n\nfrom typing import List, Optional, Tuple\nimport numpy as np\nfrom numpy.typing import ArrayLike\nfrom pydantic import BaseModel, Field\n\n\nclass CUSUMResultState(BaseModel):\n    \"\"\"\n    Results from the CUSUM procedure.\n    \"\"\"\n    statistic: float = Field()  # Current CUSUM statistic\n    alarms: List[int]  # Time points of alarms\n    alarm_stats: List[float]  # CUSUM statistics at alarm points\n    all_stats: List[float]  # Full history of statistics\n    n_alarms: int = Field(default=0, ge=0)  # Total number of alarms raised\n\nclass ConformalCUSUM:\n    \"\"\"\n    Implementation of the conformal CUSUM e-procedure as described in Section 5.\n    Includes both standard and reverse Shiryaev-Roberts modifications.\n    \"\"\"\n    def __init__(self, \n                 threshold: float = 20.0,\n                 use_sr: bool = False,\n                 truncate: bool = True,\n                 min_value: float = 1e-10):\n        if threshold <= 1:\n            raise ValueError(\"Threshold must be > 1\")\n        \n        self.threshold = threshold\n        self.use_sr = use_sr\n        self.truncate = truncate\n        self.min_value = min_value\n        \n        self.reset()\n    \n    def reset(self):\n        self._last_alarm = 0  # Time of last alarm\n        self._cusum_stat = 0.0  # Current CUSUM statistic\n        self._stats_history = []  # History of statistics\n        self._alarms = []  # Alarm times\n        self._alarm_stats = []  # Statistics at alarm times\n        \n    def update(self, e_value: float) -> CUSUMResultState:\n        # Update time step\n        t = len(self._stats_history) + 1\n        \n        # Calculate CUSUM statistic\n        if self.use_sr:\n            # Shiryaev-Roberts modification\n            sum_stat = sum(e_value * stat for stat in \n                         self._stats_history[self._last_alarm:])\n            self._cusum_stat = max(sum_stat, self.min_value if self.truncate else 0)\n        else:\n            # Standard CUSUM\n            self._cusum_stat = max(\n                self._cusum_stat * e_value,\n                self.min_value if self.truncate else 0\n            )\n        \n        # Store statistic\n        self._stats_history.append(self._cusum_stat)\n        \n        # Check for alarm\n        if self._cusum_stat >= self.threshold:\n            self._alarms.append(t)\n            self._alarm_stats.append(self._cusum_stat)\n            self._last_alarm = t\n            self._cusum_stat = 0.0  # Reset after alarm\n            \n        return CUSUMResultState(\n            statistic=self._cusum_stat,\n            alarms=self._alarms.copy(),\n            alarm_stats=self._alarm_stats.copy(),\n            all_stats=self._stats_history.copy(),\n            n_alarms=len(self._alarms)\n        )\n    \n    def get_alarm_rate(self) -> float:\n        t = len(self._stats_history)\n        return len(self._alarms) / t if t > 0 else 0.0\n\nclass EfficiencyAnalyzer:\n    \"\"\"\n    Tools for analyzing efficiency of conformal e-testing procedures\n    as described in Section 6 of the paper.\n    \"\"\"\n    def __init__(self, \n                 pre_dist: Optional[callable] = None,\n                 post_dist: Optional[callable] = None):\n        self.pre_dist = pre_dist or (lambda n: np.random.normal(0, 1, n))\n        self.post_dist = post_dist or (lambda n: np.random.normal(0.5, 1, n))\n        \n    def compute_likelihood_ratios(self, data: ArrayLike) -> np.ndarray:\n        data = np.asarray(data)\n        # Using normal distributions as default\n        pre_likelihood = np.exp(-0.5 * data**2) / np.sqrt(2 * np.pi)\n        post_likelihood = np.exp(-0.5 * (data - 0.5)**2) / np.sqrt(2 * np.pi)\n        return post_likelihood / pre_likelihood\n    \n    def analyze_decay(self, \n                     e_values: ArrayLike,\n                     change_point: int) -> Tuple[float, float]:\n        e_values = np.asarray(e_values)\n        post_change = e_values[change_point:]\n        \n        if len(post_change) < 2:\n            return 0.0, float('inf')\n            \n        # Compute log ratios\n        log_ratios = np.log(post_change[1:] / post_change[:-1])\n        \n        # Estimate decay rate\n        decay_rate = -np.mean(log_ratios)\n        decay_se = np.std(log_ratios) / np.sqrt(len(log_ratios))\n        \n        return decay_rate, decay_se\n    \n    def compute_efficiency_metrics(self, \n                                 detector: ConformalCUSUM,\n                                 n_pre: int = 1000,\n                                 n_post: int = 1000,\n                                 n_trials: int = 100) -> dict:\n        detection_delays = []\n        false_alarms = []\n        decay_rates = []\n        \n        for _ in range(n_trials):\n            # Generate data\n            pre_data = self.pre_dist(n_pre)\n            post_data = self.post_dist(n_post)\n            data = np.concatenate([pre_data, post_data])\n            \n            # Process with detector\n            detector.reset()\n            e_values = []\n            detected = False\n            \n            for x in data:\n                # Compute e-value (simplified)\n                e_value = np.exp(-0.5 * (x - 0.5)**2 + 0.5 * x**2)\n                e_values.append(e_value)\n                \n                # Update detector\n                result = detector.update(e_value)\n                \n                # Check for first detection after change point\n                if not detected and result.n_alarms > 0:\n                    last_alarm = result.alarms[-1]\n                    if last_alarm > n_pre:\n                        detection_delays.append(last_alarm - n_pre)\n                        detected = True\n                    else:\n                        false_alarms.append(last_alarm)\n            \n            # Analyze decay\n            decay_rate, _ = self.analyze_decay(e_values, n_pre)\n            decay_rates.append(decay_rate)\n        \n        return {\n            'mean_detection_delay': np.mean(detection_delays),\n            'detection_delay_std': np.std(detection_delays),\n            'false_alarm_rate': len(false_alarms) / n_trials,\n            'mean_decay_rate': np.mean(decay_rates),\n            'decay_rate_std': np.std(decay_rates)\n        }",
    "uncovered_lines": [
      37,
      60,
      62,
      75,
      76,
      77,
      78,
      89,
      90,
      100,
      101,
      104,
      106,
      107,
      108,
      113,
      114,
      116,
      117,
      120,
      123,
      124,
      126,
      133,
      134,
      135,
      137,
      139,
      140,
      141,
      144,
      145,
      146,
      148,
      150,
      151,
      154,
      157,
      158,
      159,
      160,
      161,
      163,
      166,
      167,
      169
    ]
  },
  {
    "file_path": "expectation/confseq/confidenceconfig.py",
    "code": "from enum import Enum\nfrom typing import Optional\nfrom pydantic import BaseModel, Field, ConfigDict, validator\n\n\nclass BoundaryType(str, Enum):\n    NORMAL_MIXTURE = \"normal_mixture\"\n    BETA_BINOMIAL = \"beta_binomial\" \n    GAMMA_EXPONENTIAL = \"gamma_exponential\"\n    POLY_STITCHING = \"poly_stitching\"\n    DISCRETE_MIXTURE = \"discrete_mixture\"\n\nclass EstimandType(str, Enum):\n    MEAN = \"mean\"\n    QUANTILE = \"quantile\"\n    VARIANCE = \"variance\"\n    PROPORTION = \"proportion\"\n\nclass ConfidenceSequenceConfig(BaseModel):\n    alpha: float = Field(gt=0, lt=1, default=0.05)\n    alpha_opt: float = Field(gt=0, lt=1, default=0.05)\n    v_opt: float = Field(gt=0, default=1.0)\n    c: float = Field(gt=0, default=1.0)  # Added default value\n    boundary_type: BoundaryType = Field(default=BoundaryType.NORMAL_MIXTURE)\n    model_config = ConfigDict(frozen=True)\n\n# Updated EmpiricalBernsteinConfig with more robust defaults\nclass EmpiricalBernsteinConfig(ConfidenceSequenceConfig):\n    \"\"\"\n    Configuration for Empirical Bernstein confidence sequences.\n    \n    Extends base config with bounds on the observations.\n    \"\"\"\n    lower_bound: float = Field(description=\"Lower bound on observations\")\n    upper_bound: float = Field(description=\"Upper bound on observations\")\n    boundary_type: BoundaryType = Field(\n        default=BoundaryType.NORMAL_MIXTURE,  # Changed default to more stable boundary\n        description=\"Type of boundary to use\"\n    )\n    rho: float = Field(\n        default=2.0,  # Increased default for better stability\n        gt=0,\n        description=\"Tuning parameter controlling boundary shape\"\n    )\n    \n    @validator('upper_bound')\n    def upper_bound_must_exceed_lower(cls, v: float, values: dict) -> float:\n        if 'lower_bound' in values and v <= values['lower_bound']:\n            raise ValueError('upper_bound must be greater than lower_bound')\n        return v\n",
    "uncovered_lines": [
      49
    ]
  },
  {
    "file_path": "expectation/modules/boundaries.py",
    "code": "# Translatend from https://github.com/gostevehoward/confseq boundaries.cpp\n# Howard, S. R., Waudby-Smith, I. and Ramdas, A. (2019-), ConfSeq: software for confidence sequences and uniform boundaries, https://github.com/gostevehoward/confseq [Online; accessed ].\n\n# Using numpy and scipy for math functions\n# Boosts special functions -> Scipy equivalents\n# Numpy arrays for vectorized calculations\n# Scipy optimize for root finding\n\nimport numpy as np\nfrom scipy import special, optimize\nfrom typing import Tuple, Union\nfrom numpy.typing import ArrayLike\nfrom expectation.modules.martingales import (BetaBinomialMixture, \n                                                                     OneSidedNormalMixture, \n                                                                     TwoSidedNormalMixture, \n                                                                     GammaExponentialMixture, \n                                                                     GammaPoissonMixture, \n                                                                     PolyStitchingBound,\n                                                                     EmpiricalProcessLILBound)\n\n\ndef log_beta(a: float, b: float) -> float:\n    return special.gammaln(a) + special.gammaln(b) - special.gammaln(a + b)\n\ndef log_incomplete_beta(a: float, b: float, x: float) -> float:\n    if x == 1:\n        return log_beta(a, b)\n    return np.log(special.betainc(a, b, x)) + log_beta(a, b)\n\n# Simple interface functions\ndef normal_log_mixture(s: ArrayLike, v: ArrayLike, v_opt: float, \n                      alpha_opt: float = 0.05, is_one_sided: bool = True) -> np.ndarray:\n    s, v = np.asarray(s), np.asarray(v)\n    mixture = OneSidedNormalMixture(v_opt, alpha_opt) if is_one_sided else TwoSidedNormalMixture(v_opt, alpha_opt)\n    return np.vectorize(mixture.log_superMG)(s, v)\n\n\ndef normal_mixture_bound(v: ArrayLike, alpha: float, v_opt: float,\n                        alpha_opt: float = 0.05, is_one_sided: bool = True) -> np.ndarray:\n    v = np.asarray(v)\n    mixture = OneSidedNormalMixture(v_opt, alpha_opt) if is_one_sided else TwoSidedNormalMixture(v_opt, alpha_opt)\n    return np.vectorize(lambda x: mixture.bound(x, np.log(1/alpha)))(v)\n\ndef gamma_exponential_log_mixture(s: ArrayLike, v: ArrayLike, v_opt: float,\n                                c: float, alpha_opt: float = 0.05) -> np.ndarray:\n    s, v = np.asarray(s), np.asarray(v)\n    mixture = GammaExponentialMixture(v_opt, alpha_opt, c)\n    return np.vectorize(mixture.log_superMG)(s, v)\n\ndef gamma_exponential_mixture_bound(v: ArrayLike, alpha: float, v_opt: float,\n                                  c: float, alpha_opt: float = 0.05) -> np.ndarray:\n    v = np.asarray(v)\n    mixture = GammaExponentialMixture(v_opt, alpha_opt, c)\n    return np.vectorize(lambda x: mixture.bound(x, np.log(1/alpha)))(v)\n\ndef gamma_poisson_log_mixture(s: ArrayLike, v: ArrayLike, v_opt: float,\n                            c: float, alpha_opt: float = 0.05) -> np.ndarray:\n    s, v = np.asarray(s), np.asarray(v)\n    mixture = GammaPoissonMixture(v_opt, alpha_opt, c)\n    return np.vectorize(mixture.log_superMG)(s, v)\n\ndef gamma_poisson_mixture_bound(v: ArrayLike, alpha: float, v_opt: float,\n                              c: float, alpha_opt: float = 0.05) -> np.ndarray:\n    v = np.asarray(v)\n    mixture = GammaPoissonMixture(v_opt, alpha_opt, c)\n    return np.vectorize(lambda x: mixture.bound(x, np.log(1/alpha)))(v)\n\ndef beta_binomial_log_mixture(s: ArrayLike, v: ArrayLike, v_opt: float,\n                            g: float, h: float, alpha_opt: float = 0.05,\n                            is_one_sided: bool = True) -> np.ndarray:\n    s, v = np.asarray(s), np.asarray(v)\n    mixture = BetaBinomialMixture(v_opt, alpha_opt, g, h, is_one_sided)\n    return np.vectorize(mixture.log_superMG)(s, v)\n\ndef beta_binomial_mixture_bound(v: ArrayLike, alpha: float, v_opt: float,\n                              g: float, h: float, alpha_opt: float = 0.05,\n                              is_one_sided: bool = True) -> np.ndarray:\n    v = np.asarray(v)\n    mixture = BetaBinomialMixture(v_opt, alpha_opt, g, h, is_one_sided)\n    return np.vectorize(lambda x: mixture.bound(x, np.log(1/alpha)))(v)\n\ndef poly_stitching_bound(v: ArrayLike, alpha: float, v_min: float,\n                        c: float = 0, s: float = 1.4, eta: float = 2) -> np.ndarray:\n    v = np.asarray(v)\n    bound = PolyStitchingBound(v_min, c, s, eta)\n    return np.vectorize(lambda x: bound(x, alpha))(v)\n\ndef empirical_process_lil_bound(t: Union[int, float], alpha: float,\n                               t_min: float, A: float = 0.85) -> float:\n    bound = EmpiricalProcessLILBound(alpha, t_min, A)\n    return bound(t)\n\ndef double_stitching_bound(quantile_p: float, t: float, alpha: float,\n                          t_opt: float, delta: float = 0.5,\n                          s: float = 1.4, eta: float = 2) -> float:\n    t_max_m = max(t, t_opt)\n    \n    def logit(p: float) -> float:\n        return np.log(p / (1 - p))\n    \n    def expit(l: float) -> float:\n        return 1 / (1 + np.exp(-l))\n    \n    r = (quantile_p if quantile_p >= 0.5 else \n         min(0.5, expit(logit(quantile_p) + 2 * delta * np.sqrt(t_opt * eta / t_max_m))))\n    \n    sigma_sq = r * (1 - r)\n    j = np.sqrt(t_max_m / t_opt) * abs(logit(quantile_p)) / (2 * delta) + 1\n    zeta_s = special.zeta(s)\n    \n    ell = (s * np.log(np.log(eta * t_max_m / t_opt)) + s * np.log(j) +\n           np.log(2 * zeta_s * (2 * zeta_s + 1) / (alpha * np.power(np.log(eta), s))))\n    \n    cp = (1 - 2 * quantile_p) / 3\n    k1 = (np.power(eta, 0.25) + np.power(eta, -0.25)) / np.sqrt(2)\n    k2 = (np.sqrt(eta) + 1) / 2\n    term2 = k2 * cp * ell\n    \n    return (delta * np.sqrt(eta * t_max_m * sigma_sq / t_opt) +\n            np.sqrt(k1 * k1 * sigma_sq * t_max_m * ell + term2 * term2) + term2)\n\ndef bernoulli_confidence_interval(num_successes: Union[float, int],\n                                num_trials: Union[float, int],\n                                alpha: float,\n                                t_opt: float,\n                                alpha_opt: float = 0.05) -> Tuple[float, float]:\n    threshold = np.log(1/alpha)\n    empirical_p = float(num_successes) / num_trials\n    \n    def objective(p: float) -> float:\n        if p <= 0 or p >= 1:\n            return np.inf\n        mixture = BetaBinomialMixture(p * (1 - p) * t_opt, alpha_opt, p, 1 - p, False)\n        s = (empirical_p - p) * num_trials\n        v = p * (1 - p) * num_trials\n        return mixture.log_superMG(s, v) - threshold\n    \n    # Find lower bound\n    lower_bound = 0.0\n    if empirical_p > 0:\n        lower_bound = optimize.bisect(\n            lambda p: objective(p),\n            0.0, empirical_p,\n            xtol=2**-40\n        )\n    \n    # Find upper bound\n    upper_bound = 1.0\n    if empirical_p < 1:\n        upper_bound = optimize.bisect(\n            lambda p: -objective(p),\n            empirical_p, 1.0,\n            xtol=2**-40\n        )\n    \n    return lower_bound, upper_bound",
    "uncovered_lines": [
      58,
      59,
      60,
      64,
      65,
      66
    ]
  },
  {
    "file_path": "expectation/modules/epower.py",
    "code": "import numpy as np\nfrom typing import Optional, Union\nfrom pydantic import BaseModel, Field\nfrom enum import Enum\n\nclass EPowerType(str, Enum):\n    STANDARD = \"standard\"\n    ALL_OR_NOTHING = \"all_or_nothing\"\n    OPTIMIZED = \"optimized\"\n\nclass EPowerConfig(BaseModel):\n    type: EPowerType = Field(default=EPowerType.STANDARD)\n    optimize_lambda: bool = Field(default=False)\n    grid_size: int = Field(default=100)\n    min_lambda: float = Field(default=0.0)\n    max_lambda: float = Field(default=1.0)\n\nclass EPowerResult(BaseModel):\n    e_power: float = Field(description=\"Computed e-power value\")\n    is_positive: bool = Field(description=\"Whether e-power is positive\")\n    expected_e_value: float = Field(description=\"Expected e-value\")\n    optimal_lambda: Optional[float] = Field(default=None, description=\"Optimal lambda if optimized\")\n    type: EPowerType = Field(description=\"Type of e-power calculation used\")\n\nclass EPowerCalculator:\n    \"\"\"\n    Calculator for e-power metrics.\n    \"\"\"\n\n    def __init__(self, config: Optional[EPowerConfig] = None):\n        self.config = config or EPowerConfig()\n\n    def compute(\n        self,\n        e_values: np.ndarray,\n        alternative_prob: Optional[np.ndarray] = None\n    ) -> EPowerResult:\n        if alternative_prob is None:\n            alternative_prob = np.ones(len(e_values)) / len(e_values)\n\n        if self.config.type == EPowerType.ALL_OR_NOTHING:\n            # Convert to all-or-nothing e-values\n            e_values = np.where(e_values > 1, 1/0.05, 0)  # Using standard α=0.05\n\n        # Compute base e-power\n        e_power = np.sum(alternative_prob * np.log(e_values))\n        expected_e_value = np.sum(alternative_prob * e_values)\n\n        optimal_lambda = None\n        if self.config.optimize_lambda:\n            optimal_lambda = self._optimize_lambda(e_values, alternative_prob)\n            # Transform e-values using optimal lambda\n            e_values = 1 - optimal_lambda + optimal_lambda * e_values\n            e_power = np.sum(alternative_prob * np.log(e_values))\n            expected_e_value = np.sum(alternative_prob * e_values)\n\n        return EPowerResult(\n            e_power=float(e_power),\n            is_positive=e_power > 0,\n            expected_e_value=float(expected_e_value),\n            optimal_lambda=optimal_lambda,\n            type=self.config.type\n        )\n\n    def _optimize_lambda(\n        self,\n        e_values: np.ndarray,\n        alternative_prob: np.ndarray\n    ) -> float:\n        lambdas = np.linspace(\n            self.config.min_lambda,\n            self.config.max_lambda,\n            self.config.grid_size\n        )\n\n        max_e_power = float('-inf')\n        optimal_lambda = 0\n\n        for lam in lambdas:\n            transformed_e_values = 1 - lam + lam * e_values\n            e_power = np.sum(alternative_prob * np.log(transformed_e_values))\n\n            if e_power > max_e_power:\n                max_e_power = e_power\n                optimal_lambda = lam\n\n        return optimal_lambda\n",
    "uncovered_lines": [
      43
    ]
  },
  {
    "file_path": "expectation/modules/hypothesistesting.py",
    "code": "\"\"\"\nBased on these papers:\n\nHypothesis testing with e-values, A. Ramdas, R. Wang (2024) - https://arxiv.org/pdf/2410.23614\n\nSafe Testing, P. Grünwald, R. de Heide, W.M Koolen (2019) - https://arxiv.org/pdf/1906.07801\n\"\"\"\n\nfrom typing import Optional, Callable, Sequence, ClassVar, List, Union, Tuple\nfrom abc import ABC, abstractmethod\nimport numpy as np\nfrom pydantic import BaseModel, Field, ConfigDict\nfrom numpy.typing import NDArray\nfrom enum import Enum\n\nfrom expectation.modules.epower import (\n    EPowerCalculator, EPowerConfig, EPowerType, EPowerResult\n)\n\nclass SymmetryType(str, Enum):\n    FISHER = \"fisher\"\n    SIGN = \"sign\"\n    WILCOXON = \"wilcoxon\"\n\nclass HypothesisType(str, Enum):\n    SIMPLE = \"simple\"\n    COMPOSITE = \"composite\"\n    POINT = \"point\"\n\nclass Hypothesis(BaseModel):\n    name: str\n    description: Optional[str] = None\n    type: HypothesisType\n    model_config = ConfigDict(frozen=True)\n\nclass EValueConfig(BaseModel):\n    significance_level: float = Field(gt=0, lt=1, default=0.05)\n    allow_infinite: bool = Field(default=False)\n    model_config = ConfigDict(frozen=True)\n\nclass EValueResult(BaseModel):\n    value: float = Field(ge=0)\n    significant: bool\n    sample_size: int = Field(gt=0)\n    hypothesis: Hypothesis\n    config: EValueConfig\n    timestamp: float = Field(default_factory=lambda: np.datetime64('now').astype(float))\n\nclass EValue(ABC):\n    config: ClassVar[EValueConfig] = EValueConfig()\n    \n    def __init__(self, \n                 null_hypothesis: Hypothesis,\n                 config: Optional[EValueConfig] = None):\n        self.null_hypothesis = null_hypothesis\n        self.config = config or self.config\n        self._result: Optional[EValueResult] = None\n    \n    @abstractmethod\n    def compute(self, data: NDArray) -> float:\n        \"\"\"\n        Compute the e-value for given data.\n        \"\"\"\n        pass\n    \n    def test(self, data: NDArray) -> EValueResult:\n        \"\"\"\n        Compute e-value and return detailed results.\n        \"\"\"\n        value = self.compute(data)\n        \n        if not self.config.allow_infinite and np.isinf(value):\n            raise ValueError(\"Infinite e-value detected and not allowed by config\")\n            \n        result = EValueResult(\n            value=float(value),\n            significant=value >= 1/self.config.significance_level,\n            sample_size=len(data),\n            hypothesis=self.null_hypothesis,\n            config=self.config\n        )\n        self._result = result\n        return result\n    \n    @property\n    def result(self) -> Optional[EValueResult]:\n        \"\"\"\n        Get the latest test result if available.\n        \"\"\"\n        return self._result\n\nclass LikelihoodRatioEValue(EValue):\n    class Config(BaseModel):\n        log_space: bool = Field(default=True, description=\"Compute in log space for numerical stability\")\n        model_config = ConfigDict(frozen=True)\n    \n    def __init__(self,\n                 null_hypothesis: Hypothesis,\n                 null_density: Callable[[NDArray], NDArray],\n                 alt_density: Callable[[NDArray], NDArray],\n                 config: Optional[EValueConfig] = None,\n                 lr_config: Optional[Config] = None):\n        super().__init__(null_hypothesis, config)\n        self.null_density = null_density\n        self.alt_density = alt_density\n        self.lr_config = lr_config or self.Config()\n    \n    def compute(self, data: NDArray) -> float:\n        if self.lr_config.log_space:\n            log_ratios = np.log(self.alt_density(data)) - np.log(self.null_density(data))\n            return float(np.exp(np.sum(log_ratios)))\n        else:\n            ratios = self.alt_density(data) / self.null_density(data)\n            return float(np.prod(ratios))\n\nclass EProcess(BaseModel):    \n    values: list[float] = Field(default_factory=list)\n    cumulative_value: float = Field(default=1.0)\n    total_samples: int = Field(default=0)\n    config: EValueConfig\n    \n    def update(self, e_value: float) -> float:\n        self.values.append(e_value)\n        self.cumulative_value *= e_value\n        self.total_samples += 1\n        return self.cumulative_value\n    \n    def is_significant(self, alpha: Optional[float] = None) -> bool:\n        alpha = alpha or self.config.significance_level\n        return self.cumulative_value >= 1/alpha\n\nclass UniversalEValue(EValue):\n    class Config(BaseModel):\n        split_ratio: float = Field(gt=0, lt=1, default=0.5)\n        min_samples: int = Field(ge=2, default=4)\n        model_config = ConfigDict(frozen=True)\n    \n    def __init__(self,\n                 null_hypothesis: Hypothesis,\n                 null_mle: Callable[[NDArray], Callable[[NDArray], NDArray]],\n                 alt_mle: Callable[[NDArray], Callable[[NDArray], NDArray]],\n                 config: Optional[EValueConfig] = None,\n                 ui_config: Optional[Config] = None):\n        super().__init__(null_hypothesis, config)\n        self.null_mle = null_mle\n        self.alt_mle = alt_mle\n        self.ui_config = ui_config or self.Config()\n    \n    def compute(self, data: NDArray) -> float:\n        if len(data) < self.ui_config.min_samples:\n            raise ValueError(f\"Need at least {self.ui_config.min_samples} samples\")\n            \n        split_idx = int(len(data) * self.ui_config.split_ratio)\n        D0, D1 = data[:split_idx], data[split_idx:]\n        \n        q1_hat = self.alt_mle(D1)\n        p0_hat = self.null_mle(D0)\n        \n        ratios = q1_hat(D0) / p0_hat(D0)\n        return float(np.prod(ratios))\n    \n\nclass SymmetryETest:\n    \"\"\"Implementation of nonparametric e-tests of symmetry from the paper:\n    Nonparametric E-tests of symmetry, Vovk and R. Wang (2024) - https://doi.org/10.51387/24-NEJSDS60\n    \n    This class implements the three symmetry tests discussed in the paper:\n    1. Fisher-type test (based on sum of observations)\n    2. Sign test (based on number of positive observations)\n    3. Wilcoxon signed-rank test (based on ranks of positive observations)\n    \"\"\"\n    \n    def __init__(self, \n                 test_type: SymmetryType = SymmetryType.FISHER,\n                 config: Optional[EValueConfig] = None,\n                 lambda_value: float = 0.5,\n                 e_power_config: Optional[EPowerConfig] = None):\n        self.null_hypothesis = Hypothesis(\n            name=\"Symmetry\",\n            description=\"Distribution is symmetric around 0\",\n            type=HypothesisType.COMPOSITE\n        )\n        \n        self.config = config or EValueConfig()\n        self.test_type = test_type\n        self.lambda_value = lambda_value\n        self.e_power_calculator = EPowerCalculator(e_power_config)\n        self._result = None\n        \n    def compute(self, data: NDArray) -> float:\n        if self.test_type == SymmetryType.FISHER:\n            return self._compute_fisher_e_value(data)\n        elif self.test_type == SymmetryType.SIGN:\n            return self._compute_sign_e_value(data)\n        elif self.test_type == SymmetryType.WILCOXON:\n            return self._compute_wilcoxon_e_value(data)\n        else:\n            raise ValueError(f\"Unknown test type: {self.test_type}\")\n            \n    def _compute_fisher_e_value(self, data: NDArray) -> float:\n        \"\"\"\n        Compute Fisher-type e-value (Section 4 in the paper).\n        \n        This implements equation (4.4) from the paper\n        \"\"\"\n        lambda_val = self.lambda_value\n        numerator = np.exp(lambda_val * data)\n        denominator = 0.5 * (np.exp(lambda_val * data) + np.exp(-lambda_val * data))\n        \n        e_value = np.prod(numerator / denominator)\n        \n        return float(e_value)\n    \n    def _compute_sign_e_value(self, data: NDArray) -> float:\n        \"\"\"\n        Compute Sign e-value (Section 7 in the paper).\n        \n        This implements equation (7.3) from the paper where k is the number of psitive observations\n        \"\"\"\n        lambda_val = self.lambda_value\n\n        k = np.sum(data > 0)\n        n = len(data)\n\n        e_value = np.exp(lambda_val * k) * (2 / (1 + np.exp(lambda_val)))**n\n        \n        return float(e_value)\n    \n    def _compute_wilcoxon_e_value(self, data: NDArray) -> float:\n        \"\"\"\n        Compute Wilcoxon signed-rank e-value (Section 8 in the paper).\n        \n        This implements equation (8.3) from the paper where V_n is the sum of ranks of positive observations.\n        \"\"\"\n        lambda_val = self.lambda_value\n        n = len(data)\n\n        abs_data = np.abs(data)\n        ranks = np.argsort(np.argsort(abs_data)) + 1\n \n        V_n = np.sum(ranks[data > 0])\n        \n        numerator = np.exp(lambda_val * V_n)\n        denominator_factors = np.array([1 + np.exp(lambda_val * i) for i in range(1, n+1)])\n        denominator = np.prod(2 / denominator_factors)\n        \n        e_value = numerator * denominator\n        \n        return float(e_value)\n\n    def test(self, data: NDArray) -> EValueResult:\n        value = self.compute(data)\n        \n        if not self.config.allow_infinite and np.isinf(value):\n            raise ValueError(\"Infinite e-value detected and not allowed by config\")\n            \n        result = EValueResult(\n            value=float(value),\n            significant=value >= 1/self.config.significance_level,\n            sample_size=len(data),\n            hypothesis=self.null_hypothesis,\n            config=self.config\n        )\n        self._result = result\n        return result\n    \n    @property\n    def result(self) -> Optional[EValueResult]:\n        return self._result\n    \n    def compute_e_power(self, \n                        alternative_data: NDArray, \n                        e_power_config: Optional[EPowerConfig] = None) -> EPowerResult:\n        # Compute e-values for the alternative data\n        if alternative_data.ndim == 1:\n            # Single sample\n            e_values = np.array([self.compute(alternative_data)])\n        else:\n            # Multiple samples\n            e_values = np.array([self.compute(sample) for sample in alternative_data])\n        \n        # Use the e-power calculator from epower.py\n        calculator = EPowerCalculator(e_power_config or self.e_power_calculator.config)\n        return calculator.compute(e_values)\n    \n    def get_asy_efficiency(self) -> float:\n        # Return the known asymptotic efficiencies from the paper\n        if self.test_type == SymmetryType.FISHER:\n            return 1.0  # Fisher's test has efficiency 1 (Section 6)\n        elif self.test_type == SymmetryType.SIGN:\n            return 2/np.pi  # Sign test has efficiency 2/pi\n        elif self.test_type == SymmetryType.WILCOXON:\n            return 3/np.pi  # Wilcoxon test has efficiency 3/pi\n        else:\n            return 0.0\n",
    "uncovered_lines": [
      73,
      90,
      113,
      114,
      144,
      145,
      146,
      147,
      150,
      151,
      153,
      154,
      156,
      157,
      159,
      160,
      178,
      184,
      185,
      186,
      187,
      188,
      191,
      192,
      193,
      194,
      195,
      196,
      198,
      206,
      207,
      208,
      210,
      212,
      220,
      222,
      223,
      225,
      227,
      235,
      236,
      238,
      239,
      241,
      243,
      244,
      245,
      247,
      249,
      252,
      254,
      255,
      257,
      264,
      265,
      269,
      275,
      277,
      280,
      283,
      284,
      288,
      289,
      290,
      291,
      292,
      293,
      295
    ]
  },
  {
    "file_path": "expectation/modules/martingales.py",
    "code": "\"\"\"\nBased on these papers:\n\nTime-uniform, nonparametric, nonasymptotic confidence sequences, S.R Howard, A. Ramdas, J. McAuliffe, J. Sekhon (2022) - https://arxiv.org/pdf/1810.08240\n\nMerging sequential e-values via martingales, V. Vovk, R. Wang (2024) - https://arxiv.org/pdf/2007.06382\n\nTime-uniform central limit theory and asymptotic confidence sequences, I. Waudby-Smith, D. Arbour, R. Sinha, E.H Kennedy, A. Ramdas (2021) - https://arxiv.org/pdf/2103.06476\n\"\"\"\n# Translatend from https://github.com/gostevehoward/confseq boundaries.cpp\n# Howard, S. R., Waudby-Smith, I. and Ramdas, A. (2019-), ConfSeq: software for confidence sequences and uniform boundaries, https://github.com/gostevehoward/confseq [Online; accessed ].\n\n# Using numpy and scipy for math functions\n# Boosts special functions -> Scipy equivalents\n# Numpy arrays for vectorized calculations\n# Scipy optimize for root finding\n\n\nimport numpy as np\nfrom scipy import special, optimize, stats\nfrom abc import ABC, abstractmethod\n\nclass MixtureSupermartingale(ABC):\n    \"\"\"\n    Abstract base class for mixture supermartingales.\n    \"\"\"\n\n    @abstractmethod\n    def log_superMG(self, s: float, v: float) -> float:\n        pass\n    \n    @abstractmethod\n    def s_upper_bound(self, v: float) -> float:\n        pass\n    \n    @abstractmethod\n    def bound(self, v: float, log_threshold: float) -> float:\n        pass\n\ndef find_s_upper_bound(mixture: MixtureSupermartingale, v: float, log_threshold: float) -> float:\n    trial_upper_bound = float(v)\n    for _ in range(50):\n        if mixture.log_superMG(trial_upper_bound, v) > log_threshold:\n            return trial_upper_bound\n        trial_upper_bound *= 2\n    raise RuntimeError(\"Failed to find upper limit for mixture bound\")\n\ndef find_mixture_bound(mixture: MixtureSupermartingale, v: float, log_threshold: float) -> float:\n    def root_fn(s: float) -> float:\n        return mixture.log_superMG(s, v) - log_threshold\n\n    s_upper = mixture.s_upper_bound(v)\n    if np.isinf(s_upper):\n        s_upper = find_s_upper_bound(mixture, v, log_threshold)\n    \n    if root_fn(s_upper) < 0:\n        return s_upper\n    \n    result = optimize.bisect(root_fn, 0.0, s_upper, xtol=2**-40)\n    return result\n\nclass TwoSidedNormalMixture(MixtureSupermartingale):\n    def __init__(self, v_opt: float, alpha_opt: float):\n        assert v_opt > 0\n        self.rho = self.best_rho(v_opt, alpha_opt)\n    \n    def log_superMG(self, s: float, v: float) -> float:\n        return (0.5 * np.log(self.rho / (v + self.rho)) + \n                s * s / (2 * (v + self.rho)))\n    \n    def s_upper_bound(self, v: float) -> float:\n        return np.inf\n    \n    def bound(self, v: float, log_threshold: float) -> float:\n        return np.sqrt((v + self.rho) * (np.log(1 + v/self.rho) + 2 * log_threshold))\n    \n    @staticmethod\n    def best_rho(v: float, alpha: float) -> float:\n        assert 0 < alpha < 1\n        return v / (2 * np.log(1/alpha) + np.log(1 + 2 * np.log(1/alpha)))\n\nclass OneSidedNormalMixture(MixtureSupermartingale):\n    def __init__(self, v_opt: float, alpha_opt: float):\n        self.rho = self.best_rho(v_opt, alpha_opt)\n    \n    def log_superMG(self, s: float, v: float) -> float:\n        return (0.5 * np.log(4 * self.rho / (v + self.rho)) + \n                s * s / (2 * (v + self.rho)) +\n                np.log(stats.norm.cdf(s / np.sqrt(v + self.rho))))\n    \n    def s_upper_bound(self, v: float) -> float:\n        return np.inf\n    \n    def bound(self, v: float, log_threshold: float) -> float:\n        return find_mixture_bound(self, v, log_threshold)\n    \n    @staticmethod\n    def best_rho(v: float, alpha: float) -> float:\n        return TwoSidedNormalMixture.best_rho(v, 2 * alpha)\n\nclass GammaExponentialMixture(MixtureSupermartingale):\n    def __init__(self, v_opt: float, alpha_opt: float, c: float):\n        self.rho = OneSidedNormalMixture.best_rho(v_opt, alpha_opt)\n        self.c = c\n        self.leading_constant = self._get_leading_constant()\n    \n    def _get_leading_constant(self) -> float:\n        rho_c_sq = self.rho / (self.c * self.c)\n        return (rho_c_sq * np.log(rho_c_sq) - \n                special.gammaln(rho_c_sq) - \n                np.log(special.gammainc(rho_c_sq, rho_c_sq)))\n    \n    def log_superMG(self, s: float, v: float) -> float:\n        c_sq = self.c * self.c\n        cs_v_csq = (self.c * s + v) / c_sq\n        v_rho_csq = (v + self.rho) / c_sq\n        \n        return (self.leading_constant +\n                special.gammaln(v_rho_csq) +\n                np.log(special.gammainc(v_rho_csq, cs_v_csq + self.rho/c_sq)) -\n                v_rho_csq * np.log(cs_v_csq + self.rho/c_sq) +\n                cs_v_csq)\n    \n    def s_upper_bound(self, v: float) -> float:\n        return np.inf\n    \n    def bound(self, v: float, log_threshold: float) -> float:\n        return find_mixture_bound(self, v, log_threshold)\n\nclass GammaPoissonMixture(MixtureSupermartingale):\n    def __init__(self, v_opt: float, alpha_opt: float, c: float):\n        self.rho = OneSidedNormalMixture.best_rho(v_opt, alpha_opt)\n        self.c = c\n        self.leading_constant = self._get_leading_constant()\n    \n    def _get_leading_constant(self) -> float:\n        rho_c_sq = self.rho / (self.c * self.c)\n        return (rho_c_sq * np.log(rho_c_sq) - \n                special.gammaln(rho_c_sq) - \n                np.log(special.gammaincc(rho_c_sq, rho_c_sq)))\n    \n    def log_superMG(self, s: float, v: float) -> float:\n        c_sq = self.c * self.c\n        v_rho_csq = (v + self.rho) / c_sq\n        cs_v_rho_csq = s/self.c + v_rho_csq\n        \n        return (self.leading_constant +\n                special.gammaln(cs_v_rho_csq) +\n                np.log(special.gammaincc(cs_v_rho_csq, v_rho_csq)) -\n                cs_v_rho_csq * np.log(v_rho_csq) +\n                v/c_sq)\n    \n    def s_upper_bound(self, v: float) -> float:\n        return np.inf\n    \n    def bound(self, v: float, log_threshold: float) -> float:\n        return find_mixture_bound(self, v, log_threshold)\n\nclass BetaBinomialMixture(MixtureSupermartingale):\n    def __init__(self, v_opt: float, alpha_opt: float, g: float, h: float, is_one_sided: bool):\n        assert g > 0 and h > 0\n        self.g = g\n        self.h = h\n        self.is_one_sided = is_one_sided\n        self.r = self._optimal_r(v_opt, alpha_opt)\n        self.normalizer = self._compute_normalizer()\n    \n    def _optimal_r(self, v_opt: float, alpha_opt: float) -> float:\n        rho = (OneSidedNormalMixture if self.is_one_sided else TwoSidedNormalMixture).best_rho(v_opt, alpha_opt)\n        return max(rho - self.g * self.h, 1e-3 * self.g * self.h)\n    \n    def _compute_normalizer(self) -> float:\n        x = self.h / (self.g + self.h) if self.is_one_sided else 1\n        return log_incomplete_beta(\n            self.r / (self.g * (self.g + self.h)),\n            self.r / (self.h * (self.g + self.h)),\n            x\n        )\n    \n    def log_superMG(self, s: float, v: float) -> float:\n        x = self.h / (self.g + self.h) if self.is_one_sided else 1\n        return (\n            v / (self.g * self.h) * np.log(self.g + self.h) -\n            ((v + self.h * s) / (self.h * (self.g + self.h))) * np.log(self.g) -\n            ((v - self.g * s) / (self.g * (self.g + self.h))) * np.log(self.h) +\n            log_incomplete_beta(\n                (self.r + v - self.g * s) / (self.g * (self.g + self.h)),\n                (self.r + v + self.h * s) / (self.h * (self.g + self.h)),\n                x\n            ) - self.normalizer\n        )\n    \n    def s_upper_bound(self, v: float) -> float:\n        return v / self.g\n    \n    def bound(self, v: float, log_threshold: float) -> float:\n        return find_mixture_bound(self, v, log_threshold)\n\nclass PolyStitchingBound:\n    def __init__(self, v_min: float, c: float, s: float, eta: float):\n        assert v_min > 0\n        self.v_min = v_min\n        self.c = c\n        self.s = s\n        self.eta = eta\n        self.k1 = (np.power(eta, 0.25) + np.power(eta, -0.25)) / np.sqrt(2)\n        self.k2 = (np.sqrt(eta) + 1) / 2\n        self.A = np.log(special.zeta(s) / np.power(np.log(eta), s))\n    \n    def __call__(self, v: float, alpha: float) -> float:\n        use_v = max(v, self.v_min)\n        ell = (self.s * np.log(np.log(self.eta * use_v / self.v_min)) + \n               self.A + np.log(1/alpha))\n        term2 = self.k2 * self.c * ell\n        return np.sqrt(self.k1 * self.k1 * use_v * ell + term2 * term2) + term2\n\nclass EmpiricalProcessLILBound:\n    def __init__(self, alpha: float, t_min: float, A: float):\n        assert A > 1/np.sqrt(2)\n        assert t_min >= 1\n        assert 0 < alpha < 1\n        self.t_min = t_min\n        self.A = A\n        self.C = self._find_optimal_C(alpha)\n    \n    def __call__(self, t: float) -> float:\n        if t < self.t_min:\n            return np.inf\n        return self.A * np.sqrt((np.log(1 + np.log(t / self.t_min)) + self.C) / t)\n    \n    def _find_optimal_C(self, alpha: float) -> float:\n        def error_bound(C: float, eta: float) -> float:\n            gamma_sq = 2/eta * np.power(self.A - np.sqrt(2 * (eta - 1) / C), 2)\n            if gamma_sq <= 1:\n                return np.inf\n            return 4 * np.exp(-gamma_sq * C) * (1 + 1/((gamma_sq - 1) * np.log(eta)))\n        \n        def optimize_eta(C: float) -> float:\n            def objective(eta: float) -> float:\n                return np.sqrt(eta/2) + np.sqrt(2 * (eta - 1) / C) - self.A\n            \n            eta_result = optimize.bisect(objective, 1.0, 2 * self.A * self.A)\n            return optimize.minimize_scalar(\n                lambda eta: error_bound(C, eta),\n                bounds=(1.0, eta_result),\n                method='bounded'\n            ).fun\n        \n        def objective(C: float) -> float:\n            return optimize_eta(C) - alpha\n        \n        C_result = optimize.root_scalar(\n            objective,\n            bracket=(5.0, 100.0),\n            method='bisect'\n        )\n        return C_result.root\n    \n\ndef log_beta(a: float, b: float) -> float:\n    return special.gammaln(a) + special.gammaln(b) - special.gammaln(a + b)\n\ndef log_incomplete_beta(a: float, b: float, x: float) -> float:\n    if x == 1:\n        return log_beta(a, b)\n    return np.log(special.betainc(a, b, x)) + log_beta(a, b)",
    "uncovered_lines": [
      46,
      72,
      132,
      133,
      134,
      137,
      138,
      143,
      144,
      145,
      147,
      154,
      157,
      228,
      235
    ]
  },
  {
    "file_path": "expectation/modules/quantiletest.py",
    "code": "\"\"\"\nReferences:\nSequential estimation of quantiles with applications to A/B testing and best-arm identification,\nSteven R. Howard and Aaditya Ramdas (2022 version 5) - https://arxiv.org/pdf/1906.09712\n\nhttps://github.com/gostevehoward/confseq quantiles.cpp\n\n\"\"\"\n\n\nimport numpy as np\nfrom typing import Tuple, Callable\nfrom scipy import optimize\nfrom expectation.modules.orderstatistics import OrderStatisticInterface\nfrom expectation.modules.martingales import BetaBinomialMixture\n\n\nclass QuantileABTest:\n    \"\"\"\n    Implementation of quantile A/B testing.\n    \"\"\"\n    \n    def __init__(self, quantile_p: float, t_opt: int, alpha_opt: float,\n                 arm1_os: OrderStatisticInterface, arm2_os: OrderStatisticInterface):\n        assert 0 < quantile_p < 1\n        self.quantile_p = quantile_p\n        self.mixture = BetaBinomialMixture(\n            t_opt * quantile_p * (1 - quantile_p),\n            alpha_opt, quantile_p, 1 - quantile_p, False\n        )\n        self.arm1_os = arm1_os\n        self.arm2_os = arm2_os\n    \n    def p_value(self) -> float:\n        return min(1.0, np.exp(-self.log_superMG_lower_bound()))\n    \n    def log_superMG_lower_bound(self) -> float:\n        arm1_G = self.get_G_fn(1)\n        arm2_G = self.get_G_fn(2)\n        \n        if arm1_G[1] <= arm2_G[1]:  # Compare minimum_end_x\n            return self.find_log_superMG_lower_bound(arm1_G, arm2_G, 2)\n        else:\n            return self.find_log_superMG_lower_bound(arm2_G, arm1_G, 1)\n    \n    def get_G_fn(self, arm: int) -> Tuple[Callable[[float], float], float, float]:\n        def objective(a: float) -> float:\n            return self.arm_log_superMG(arm, a)\n        \n        # Find minimizer using Brent's method\n        minimizer = optimize.minimize_scalar(objective, bounds=(0, 1), method='bounded').x\n        \n        N = self.order_stats(arm).size()\n        x_lower = self.order_stats(arm).get_order_statistic(int(np.ceil(minimizer * N)))\n        x_upper = self.order_stats(arm).get_order_statistic(int(np.floor(minimizer * N)) + 1)\n        \n        def G_callable(x: float) -> float:\n            if x < x_lower:\n                prop_below = self.order_stats(arm).count_less_or_equal(x) / N\n            elif x > x_upper:\n                prop_below = self.order_stats(arm).count_less(x) / N\n            else:\n                prop_below = minimizer\n            return self.arm_log_superMG(arm, prop_below)\n        \n        return G_callable, x_lower, x_upper\n    \n    def find_log_superMG_lower_bound(\n        self,\n        first_arm_G: Tuple[Callable[[float], float], float, float],\n        second_arm_G: Tuple[Callable[[float], float], float, float],\n        second_arm: int\n    ) -> float:\n        G1, _, _ = first_arm_G\n        G2, x2_lower, x2_upper = second_arm_G\n        \n        def objective(x: float) -> float:\n            return G1(x) + G2(x)\n        \n        # Calculate minimum value at endpoints\n        min_value = min(objective(x2_lower), objective(x2_upper))\n        \n        # Check all order statistics between x2_lower and x2_upper\n        start_index = self.order_stats(second_arm).count_less_or_equal(x2_lower)\n        end_index = self.order_stats(second_arm).count_less_or_equal(x2_upper)\n        \n        for i in range(max(1, start_index), end_index + 1):\n            x = self.order_stats(second_arm).get_order_statistic(i)\n            value = objective(x)\n            min_value = min(min_value, value)\n        \n        return min_value\n    \n    def arm_log_superMG(self, arm: int, prop_below: float) -> float:\n        N = self.order_stats(arm).size()\n        s = (prop_below - self.quantile_p) * N\n        v = self.quantile_p * (1 - self.quantile_p) * N\n        return self.mixture.log_superMG(s, v)\n    \n    def order_stats(self, arm: int) -> OrderStatisticInterface:\n        assert arm in (1, 2)\n        return self.arm1_os if arm == 1 else self.arm2_os",
    "uncovered_lines": [
      59
    ]
  },
  {
    "file_path": "expectation/parametric/ttest_universal.py",
    "code": "\"\"\"\nSequential t-tests and confidence sequences for Gaussian means with unknown variance.\n\nBased on the paper: \"Anytime-valid t-tests and confidence sequences for Gaussian means\nwith unknown variance\" by Hongjian Wang and Aaditya Ramdas (2024).\n\nThis module implements two main approaches:\n1. Universal inference t-test e-processes (Theorem 3.2)\n2. Scale invariant likelihood ratio t-test martingales (Theorem 4.9)\n\n\n-- MIXTURE BASED T-TEST --\nNull: mu = 0\nAlternative: mu != 0 or mu > 0\nBase martingales: scale invariant likelihood ratio\nParametrized by: theta = mu / sigma, the alternative standardized mean\nTest process: log supermartingale in this code\n\nTODO: Look at Alexander Ly's R package \"safestats\" for the Bayesian connection via\nGrunwald's paper \"Safe testing\". The package implements safe t-test and z-test with confidence sequences.\nSome parts to look at for the \"Safe testing\" concept:\n\n- Extended to general Bayes factors with tuned priors for optimality\n- Test supermartingales appear via conditional e-values called \"safe under optional continuation\"\n- Design plan for testing\n\nPing: @alexanderly good entrypoint\n\n\"\"\"\n\nimport numpy as np\nfrom typing import Union, Tuple\n\nfrom expectation.modules.martingales import MixtureSupermartingale\nfrom expectation.seqtest.sequential_e_testing import SequentialTest, TestType, AlternativeType\nfrom expectation.confseq.confidencesequence import ConfidenceSequence, ConfidenceSequenceConfig\n\n\nclass TtestGaussianMixtureMartingale(MixtureSupermartingale):\n    \"\"\"\n    Implementation of t-test martingale using Gaussian mixture of scale invariant likelihood ratios.\n    \n    Based on Theorem 4.9 from Wang and Ramdas (2024): \"Anytime-valid t-tests and confidence \n    sequences for Gaussian means with unknown variance\"\n    \"\"\"\n    \n    def __init__(self, prior_precision: float = 1.0):\n        \"\"\"\n        Initialize t-test martingale with Gaussian mixture.\n        \n        Args:\n            prior_precision: Precision parameter c² for the Gaussian prior N(0,c⁻²)\n        \"\"\"\n        self.c_squared = prior_precision**2\n        \n    def log_superMG(self, s: float, v: float) -> float:\n        n = 1\n        if hasattr(s, \"__len__\"):\n            n = len(s)\n        \n        # Scale invariant t-test martingale from Theorem 4.9\n        # G_n^(c) = sqrt(c^2 / (n+c^2)) * ((n + c^2) * V_n / ((n + c^2) * V_n - S_n^2))^(n/2)\n        \n        log_first_term = 0.5 * np.log(self.c_squared / (n + self.c_squared))\n        log_second_term = (n/2) * np.log((n + self.c_squared) * v / ((n + self.c_squared) * v - s**2)) # -inf problem\n        \n        return (log_first_term + log_second_term) # Log t-test martingale\n    \n    def s_upper_bound(self, v: float) -> float:\n        n = 1\n        if hasattr(v, \"__len__\"):\n            n = len(v)\n        return np.sqrt((n + self.c_squared) * v)\n    \n    def bound(self, v: float, log_threshold: float) -> float:\n        \"\"\"\n        Calculate bound given v and log threshold.\n        \n        This computes the confidence sequence radius from Theorem 4.9.\n        \"\"\"\n        n = 1\n        if hasattr(v, \"__len__\"):\n            n = len(v)\n        \n        alpha = np.exp(-log_threshold)\n        \n        # Compute the radius formula from Theorem 4.9\n        # TODO: move to confidence sequence module\n        denominator = (alpha**(2*self.c_squared/(n+self.c_squared))**(1/n)) * (n + self.c_squared) - self.c_squared\n        \n        if denominator <= 0:\n            return float('inf')\n            \n        radius_squared = (n + self.c_squared) * (1 - (alpha**(2*self.c_squared/(n+self.c_squared)))**(1/n)) / denominator * v\n        \n        return np.sqrt(radius_squared)\n\n\nclass TtestFlatMixtureMartingale(MixtureSupermartingale):\n    \"\"\"\n    Implementation of t-test extended martingale using flat mixture of scale invariant likelihood ratios.\n    \n    Based on Theorem 4.7 from Wang and Ramdas (2024): \"Anytime-valid t-tests and confidence \n    sequences for Gaussian means with unknown variance\"\n    \"\"\"\n    \n    def log_superMG(self, s: float, v: float) -> float:\n        n = 1\n        if hasattr(s, \"__len__\"):\n            n = len(s)\n        \n        # Compute the extended martingale from Theorem 4.7\n        # H_n = sqrt(2 * pi / n) * ((n * V_n) / (n * V_n - S_n^2))^(n/2)\n        \n        log_first_term = 0.5 * np.log((2 * np.pi) / n)\n        log_second_term = (n/2) * np.log( (n * v )/ (n * v - s**2))\n        \n        return (log_first_term + log_second_term) # Log t-test extended martingale\n    \n    def s_upper_bound(self, v: float) -> float:\n        n = 1\n        if hasattr(v, \"__len__\"):\n            n = len(v)\n        return np.sqrt(n * v)\n    \n    def bound(self, v: float, log_threshold: float) -> float:\n        \"\"\"\n        Calculate bound given v and log threshold.\n        \n        This computes the confidence sequence radius based on Lai's t-CS (Theorem 4.1).\n        \"\"\"\n        n = 1\n        if hasattr(v, \"__len__\"):\n            n = len(v)\n        m = max(2, n)  # Starting time m ≥ 2\n        \n        # Calculate alpha from log_threshold\n        alpha = np.exp(-log_threshold)\n        \n        # We need to solve for 'a' from the equation 2(1 - F_{m-1}(a) + af_{m-1}(a)) = alpha\n        # This is a numerical approximation based on eq. (21) in the paper\n        a = alpha**(-1/(m-1))\n        \n        # Calculate b and ξ_n from eq. (16) and (17)\n        b = (1/m) * (1 + a/2 * (m-1)/m)\n        radius = np.sqrt(v) * ((b*n)**(1/n) - 1)\n        \n        return radius\n\n# TODO: this is not usable yet, need to update the TestType config in sequential_e_testing.py\ndef setup_ttest(test: SequentialTest):\n\n    original_setup = test._setup_evaluator\n    \n    def new_setup_evaluator():\n        if test.test_type == TestType.TTEST:\n\n            ttest_method = test.config.get(\"ttest_method\", \"universal_inference\")\n            mixture_type = test.config.get(\"mixture_type\", \"gaussian\")\n            prior_precision = test.config.get(\"prior_precision\", 1.0)\n            \n\n            test._mu_estimators = [0.0]\n            test._sigma_estimators = [1.0]\n            \n            if ttest_method == \"universal_inference\":\n                def calculator(data):\n                    \n                    mean = np.mean(data)\n                    std = max(np.std(data, ddof=1), 1e-6)\n                    test._mu_estimators.append(mean)\n                    test._sigma_estimators.append(std)\n                    \n                    \n                    data_centered = data - test.null_value\n                    s = np.sum(data_centered)\n                    v = np.sum(data_centered**2)\n                    n = len(data)\n                    \n                    # Calculate e-value based on Theorem 3.2 or 3.4\n                    if test.alternative == AlternativeType.TWO_SIDED:\n                        \n                        x_squared_bar = v / n\n                        \n                        # Calculate e-process using the formula from Theorem 3.2\n                        log_process = (n/2) * np.log(x_squared_bar) + (n/2)\n                        \n                        \n                        log_process -= np.sum(np.log(test._sigma_estimators[-n-1:-1]))\n                        \n                        \n                        standardized_terms = np.zeros(n)\n                        for i in range(n):\n                            standardized_terms[i] = -0.5 * ((data_centered[i] - test._mu_estimators[-n-1+i])/test._sigma_estimators[-n-1+i])**2\n                        \n                        log_process += np.sum(standardized_terms)\n                        return np.exp(log_process)\n                    \n                    else:  # One-sided test\n                        # Implement the one-sided test (Theorem 3.4)\n                        x_squared_bar = v / n\n                        mu_bar = np.mean(data_centered)\n                        mu_bar_neg = min(0, mu_bar)\n                        adjusted_x_squared_bar = x_squared_bar - mu_bar_neg**2\n                        \n                        log_process = (n/2) * np.log(adjusted_x_squared_bar) + (n/2)\n                        \n                        \n                        log_process -= np.sum(np.log(test._sigma_estimators[-n-1:-1]))\n                        \n                        \n                        standardized_terms = np.zeros(n)\n                        for i in range(n):\n                            standardized_terms[i] = -0.5 * ((data_centered[i] - test._mu_estimators[-n-1+i])/test._sigma_estimators[-n-1+i])**2\n                        \n                        log_process += np.sum(standardized_terms)\n                        \n                        \n                        if test.alternative == AlternativeType.LESS:\n                            return np.exp(log_process.conjugate() if np.iscomplexobj(log_process) else log_process)\n                        return np.exp(log_process)\n                \n                test.e_calculator = calculator\n                \n            else:  # scale_invariant\n                if mixture_type == \"gaussian\":\n                    mixture = TtestGaussianMixtureMartingale(prior_precision=prior_precision)\n                else:\n                    mixture = TtestFlatMixtureMartingale()\n                \n                def calculator(data):\n                    data_centered = data - test.null_value\n                    s = np.sum(data_centered)\n                    v = np.sum(data_centered**2)\n                    \n                    \n                    if test.alternative == AlternativeType.TWO_SIDED:\n                        return np.exp(mixture.log_superMG(s, v))\n                    \n                    elif test.alternative == AlternativeType.GREATER:\n                        # For greater alternative, we need the semi-one-sided form (Theorem 4.11)\n                        neg_s = min(0, s)\n                        full_value = np.exp(mixture.log_superMG(s, v))\n                        neg_term = np.exp(mixture.log_superMG(neg_s, v))\n                        return 2 * (full_value - neg_term)\n                    \n                    else:  # AlternativeType.LESS\n                        pos_s = max(0, s)\n                        full_value = np.exp(mixture.log_superMG(-s, v))\n                        pos_term = np.exp(mixture.log_superMG(pos_s, v))\n                        return 2 * (full_value - pos_term)\n                \n                test.e_calculator = calculator\n        else:\n            original_setup()\n    \n    \n    test._setup_evaluator = new_setup_evaluator\n    \n    test._setup_evaluator()\n    \n    return test\n\n# TODO: move to confidence sequence module\nclass TtestConfidenceSequence(ConfidenceSequence):\n    def __init__(self, config: ConfidenceSequenceConfig, method: str = \"universal_inference\", \n                 prior_precision: float = 1.0):\n        super().__init__(config)\n        self.method = method\n        self.prior_precision = prior_precision\n        \n        self.mu_estimators = [0.0]\n        self.sigma_estimators = [1.0]\n        \n        if self.method == \"gaussian_mixture\":\n            self.mixture = TtestGaussianMixtureMartingale(prior_precision=prior_precision)\n        elif self.method == \"flat_mixture\":\n            self.mixture = TtestFlatMixtureMartingale()\n    \n    def update(self, new_data: np.ndarray) -> Tuple[float, float]:\n        result = super().update(new_data)\n        \n        if self.method == \"universal_inference\":\n            mean = np.mean(new_data)\n            std = max(np.std(new_data, ddof=1), 1e-6)\n            self.mu_estimators.append(mean)\n            self.sigma_estimators.append(std)\n            \n            # Compute confidence bounds using Theorem 3.2\n            n = self.state.n_samples\n            mu_bar = self.state.running_mean\n            x_squared_bar = self.state.sum_squares / n\n            \n            # Compute W_n from equation (10) in Theorem 3.2\n            log_sum_terms = 0\n            for i in range(min(n, len(self.mu_estimators)-1)):\n                log_sum_terms += np.log(self.sigma_estimators[i]**2) + ((new_data[i] - self.mu_estimators[i])/self.sigma_estimators[i])**2\n            \n            W = (1/self.config.alpha**(2/n)) * np.exp(log_sum_terms/n)\n            \n            radius = np.sqrt(max(0, mu_bar**2 - x_squared_bar + W))\n            \n        else:  # scale_invariant methods\n            # Compute radius using mixture bounds\n            n = self.state.n_samples\n            s_squared = self.state.variance_estimate or 0\n            \n            log_threshold = np.log(1/self.config.alpha)\n            radius = self.mixture.bound(s_squared * n, log_threshold)\n        \n        # Return confidence sequence (interval)\n        return mu_bar - radius, mu_bar + radius\n\n\ndef create_ttest(null_value: float = 0.0, \n                alternative: Union[str, AlternativeType] = \"two_sided\",\n                method: str = \"universal_inference\",\n                mixture_type: str = \"gaussian\",\n                prior_precision: float = 1.0,\n                alpha: float = 0.05) -> SequentialTest:\n    config = {\n        \"ttest_method\": method,\n        \"mixture_type\": mixture_type,\n        \"prior_precision\": prior_precision\n    }\n\n    test = SequentialTest(\n        test_type=TestType.TTEST,\n        null_value=null_value,\n        alternative=alternative,\n        config=config\n    )\n\n    return setup_ttest(test)\n\n\ndef create_ttest_confidence_sequence(alpha: float = 0.05,\n                                    method: str = \"universal_inference\",\n                                    prior_precision: float = 1.0) -> TtestConfidenceSequence:\n    config = ConfidenceSequenceConfig(alpha=alpha)\n\n    return TtestConfidenceSequence(config, method=method, prior_precision=prior_precision)",
    "uncovered_lines": [
      59,
      83,
      94,
      96,
      110,
      132,
      133,
      134,
      135,
      138,
      142,
      145,
      146,
      148,
      153,
      155,
      156,
      158,
      159,
      160,
      163,
      164,
      166,
      167,
      169,
      170,
      171,
      172,
      175,
      176,
      177,
      178,
      181,
      183,
      186,
      189,
      192,
      193,
      194,
      196,
      197,
      201,
      202,
      203,
      204,
      206,
      209,
      212,
      213,
      214,
      216,
      219,
      220,
      221,
      223,
      226,
      227,
      229,
      231,
      232,
      233,
      234,
      237,
      238,
      240,
      242,
      243,
      244,
      245,
      248,
      249,
      250,
      251,
      253,
      255,
      258,
      260,
      262,
      269,
      270,
      272,
      273,
      275,
      276,
      277,
      278,
      281,
      283,
      284,
      285,
      286,
      287,
      290,
      291,
      292,
      295,
      296,
      297,
      299,
      301,
      305,
      306,
      308,
      309,
      312,
      334
    ]
  },
  {
    "file_path": "expectation/seqtest/sequential_e_testing.py",
    "code": "from enum import Enum\nfrom typing import Optional, Union, List\nimport numpy as np\nfrom pydantic import BaseModel, Field\nimport pandas as pd\n\nfrom expectation.modules.hypothesistesting import (\n    Hypothesis, HypothesisType, EValueConfig, EProcess, \n    LikelihoodRatioEValue\n)\n\nfrom expectation.modules.martingales import (\n    BetaBinomialMixture, OneSidedNormalMixture, \n    TwoSidedNormalMixture, GammaExponentialMixture\n)\n\nfrom expectation.modules.orderstatistics import (\n    StaticOrderStatistics\n)\nfrom expectation.modules.quantiletest import (\n    QuantileABTest\n)\n\nfrom expectation.modules.epower import EPowerCalculator, EPowerConfig, EPowerType\n\nclass TestType(str, Enum):\n    \"\"\"Types of sequential tests available.\"\"\"\n    MEAN = \"mean\"\n    QUANTILE = \"quantile\"\n    VARIANCE = \"variance\"\n    PROPORTION = \"proportion\"\n\nclass AlternativeType(str, Enum):\n    \"\"\"Types of alternative hypotheses.\"\"\"\n    TWO_SIDED = \"two_sided\"\n    GREATER = \"greater\"\n    LESS = \"less\"\n\nclass SequentialTestResult(BaseModel):\n    \"\"\"Results from a sequential test.\"\"\"\n    reject_null: bool\n    e_value: float\n    e_process: EProcess\n    sample_size: int\n    p_value: Optional[float] = None\n    confidence_bounds: Optional[tuple[float, float]] = None\n    test_type: TestType\n    alternative: AlternativeType\n    timestamp: float = Field(default_factory=lambda: np.datetime64('now').astype(float))\n    e_power: Optional[float] = None\n    e_power_is_positive: Optional[bool] = None\n    optimal_lambda: Optional[float] = None\n    \n    class Config:\n        arbitrary_types_allowed = True\n\nclass SequentialTest:\n    \"\"\"\n    User-friendly interface for sequential testing using e-values and e-processes.\n    \n    Examples:\n    --------\n    # Mean test\n    >>> test = SequentialTest(test_type=\"mean\", \n                             null_value=0, \n                             alternative=\"greater\")\n    >>> result = test.update([1.2, 0.8, 1.5])\n    >>> print(f\"Reject null: {result.reject_null}\")\n    \n    # Proportion test with custom config\n    >>> config = EValueConfig(significance_level=0.01, allow_infinite=False)\n    >>> prop_test = SequentialTest(test_type=\"proportion\",\n                                  null_value=0.5,\n                                  alternative=\"two_sided\",\n                                  config=config)\n    >>> result = prop_test.update([1, 0, 1, 1, 0])\n    \n    # Quantile test\n    >>> quant_test = SequentialTest(test_type=\"quantile\",\n                                   quantile=0.5,\n                                   null_value=10)\n    >>> result = quant_test.update([8, 12, 9, 11])\n    \"\"\"\n    \n    def __init__(\n        self,\n        test_type: Union[TestType, str],\n        null_value: float,\n        alternative: Union[AlternativeType, str] = \"two_sided\",\n        quantile: Optional[float] = None,\n        config: Optional[EValueConfig] = None,\n        e_power_config: Optional[EPowerConfig] = None\n    ):\n        \"\"\"\n        Initialize sequential test.\n        \n        Parameters:\n        -----------\n        test_type : str\n            Type of test to perform (\"mean\", \"quantile\", \"variance\", \"proportion\")\n        null_value : float\n            Null hypothesis value\n        alternative : str, optional\n            Alternative hypothesis type (\"two_sided\", \"greater\", \"less\")\n        quantile : float, optional\n            Quantile to test (required for quantile tests)\n        config : EValueConfig, optional\n            Configuration for e-values and testing\n        \"\"\"\n        self.test_type = TestType(test_type)\n        self.alternative = AlternativeType(alternative)\n        self.null_value = null_value\n        self.quantile = quantile\n        self.config = config or EValueConfig()\n        self.history = []\n        self.e_power_calculator = EPowerCalculator(e_power_config)\n        self.e_values_history = []\n        \n        # Validate parameters\n        if self.test_type == TestType.QUANTILE and quantile is None:\n            raise ValueError(\"Quantile parameter required for quantile tests\")\n        if self.test_type == TestType.PROPORTION and not (0 <= null_value <= 1):\n            raise ValueError(\"Null value must be between 0 and 1 for proportion tests\")\n        \n        # Create null hypothesis\n        self.null_hypothesis = Hypothesis(\n            name=f\"{self.test_type.value.title()} Test\",\n            description=f\"H0: {self.test_type.value} = {self.null_value}\",\n            type=HypothesisType.SIMPLE\n        )\n        \n        # Initialize e-value calculator and e-process\n        self._setup_evaluator()\n        self.e_process = EProcess(config=self.config)\n        \n        # Initialize state\n        self._reset_state()\n    \n    def _reset_state(self):\n        self.data = []\n        self.n_samples = 0\n    \n    def _setup_evaluator(self):\n        if self.test_type == TestType.MEAN:\n            # Using normal mixture directly\n            mixture = (TwoSidedNormalMixture if self.alternative == AlternativeType.TWO_SIDED \n                    else OneSidedNormalMixture)(\n                v_opt=1.0,  # Can be optimized\n                alpha_opt=self.config.significance_level\n            )\n            \n            def calculator(data):\n                # For mean test:\n                n = len(data)\n                batch_mean = np.mean(data)\n                # s should be sqrt(n) * (mean - null_value) for proper scaling\n                s = np.sqrt(n) * (batch_mean - self.null_value)\n                v = 1.0  # Using standard normal scaling\n                return np.exp(mixture.log_superMG(s, v))\n                \n            self.e_calculator = calculator\n                \n        elif self.test_type == TestType.PROPORTION:\n            mixture = BetaBinomialMixture(\n                v_opt=self.null_value * (1 - self.null_value),\n                alpha_opt=self.config.significance_level,\n                g=self.null_value,\n                h=1 - self.null_value,\n                is_one_sided=self.alternative != AlternativeType.TWO_SIDED\n            )\n            \n            def calculator(data):\n                s = np.sum(data - self.null_value)\n                v = len(data) * self.null_value * (1 - self.null_value)\n                return np.exp(mixture.log_superMG(s, v))\n                \n            self.e_calculator = calculator\n\n        # TODO: This works but is it properly done? Version 1.    \n        elif self.test_type == TestType.VARIANCE:\n            # Using gamma-exponential mixture\n            mixture = GammaExponentialMixture(\n                v_opt=self.null_value,\n                alpha_opt=self.config.significance_level,\n                c=np.sqrt(self.null_value)\n            )\n            \n            def calculator(data):\n                n = len(data)\n                centered_data = data - np.mean(data)\n                s = np.sum(centered_data**2) - self.null_value * n\n                if self.alternative == AlternativeType.TWO_SIDED:\n                    return max(np.exp(mixture.log_superMG(s, n)), \n                            np.exp(mixture.log_superMG(-s, n)))\n                elif self.alternative == AlternativeType.LESS:\n                    return np.exp(mixture.log_superMG(-s, n))\n                else:  # GREATER\n                    return np.exp(mixture.log_superMG(s, n))\n                    \n            self.e_calculator = calculator\n                \n        elif self.test_type == TestType.QUANTILE:\n            self.e_calculator = None\n            \n        # TODO: Not clear which edge cases below is preferable for. Version 1 above uses direct mixture.    \n        # elif self.test_type == TestType.VARIANCE:\n        #     # Testing variance with gamma-exponential mixture\n        #     mixture = GammaExponentialMixture(\n        #         v_opt=self.null_value,  # baseline variance\n        #         alpha_opt=self.config.significance_level,\n        #         c=np.sqrt(self.null_value)  # scale parameter\n        #     )\n            \n        #     def null_density(x):\n        #         n = len(x)\n        #         centered_data = x - np.mean(x)\n        #         s = np.sum(centered_data**2) - self.null_value * n\n        #         return np.exp(mixture.log_superMG(s, n))\n            \n        #     def alt_density(x):\n        #         if self.alternative == AlternativeType.TWO_SIDED:\n        #             n = len(x)\n        #             centered_data = x - np.mean(x)\n        #             s_upper = np.sum(centered_data**2) - self.null_value * n\n        #             s_lower = -s_upper\n        #             return max(np.exp(mixture.log_superMG(s_upper, n)), \n        #                     np.exp(mixture.log_superMG(s_lower, n)))\n        #         elif self.alternative == AlternativeType.GREATER:\n        #             n = len(x)\n        #             centered_data = x - np.mean(x)\n        #             s = np.sum(centered_data**2) - self.null_value * n\n        #             return np.exp(mixture.log_superMG(s, n))\n        #         else:  # LESS\n        #             n = len(x)\n        #             centered_data = x - np.mean(x)\n        #             s = -(np.sum(centered_data**2) - self.null_value * n)\n        #             return np.exp(mixture.log_superMG(s, n))\n            \n        #     evaluator = LikelihoodRatioEValue(\n        #         null_hypothesis=self.null_hypothesis,\n        #         null_density=null_density,\n        #         alt_density=alt_density,\n        #         config=self.config\n        #     )\n\n        #     def calculator(data):\n        #         result = evaluator.test(data)\n        #         return result.value\n            \n        #     self.e_calculator = calculator\n    \n    def update(self, new_data: Union[List, np.ndarray, pd.Series]) -> SequentialTestResult:\n        new_data = np.asarray(new_data)\n        \n        # Update state\n        self.data.extend(new_data)\n        self.n_samples += len(new_data)\n        \n        # Special handling for quantile test\n        if self.test_type == TestType.QUANTILE:\n            if self.e_calculator is None:\n                self.e_calculator = QuantileABTest(\n                    quantile_p=self.quantile,\n                    t_opt=len(new_data),\n                    alpha_opt=self.config.significance_level,\n                    arm1_os=StaticOrderStatistics(new_data),\n                    arm2_os=StaticOrderStatistics([self.null_value])\n                )\n            e_value = np.exp(-self.e_calculator.log_superMG_lower_bound())\n        else:\n            # Directly compute e-value using mixture calculator\n            e_value = self.e_calculator(new_data)\n        \n        # Update e-process\n        self.e_process.update(e_value)\n    \n        \n                # Store e-value in history\n        self.e_values_history.append(e_value)\n\n        # Compute e-power if we have enough data\n        e_power_result = None\n        if len(self.e_values_history) > 1:\n            e_power_result = self.e_power_calculator.compute(\n                np.array(self.e_values_history)\n            )\n\n        history_entry = {\n            'step': len(self.history) + 1,\n            'observations': new_data.tolist(),\n            'eValue': e_value,\n            'ePower': e_power_result.e_power if e_power_result else None,\n            'cumulativeEValue': self.e_process.cumulative_value,\n            'rejectNull': self.e_process.is_significant(),\n            'timestamp': np.datetime64('now').astype(float),\n            'pValue': 1/self.e_process.cumulative_value if self.e_process.cumulative_value > 1 else 1.0\n        }\n        self.history.append(history_entry)\n        \n        return SequentialTestResult(\n            reject_null=self.e_process.is_significant(),\n            e_value=e_value,\n            e_process=self.e_process,\n            sample_size=self.n_samples,\n            p_value=1/self.e_process.cumulative_value if self.e_process.cumulative_value > 1 else 1.0,\n            test_type=self.test_type,\n            alternative=self.alternative,\n            e_power=e_power_result.e_power if e_power_result else None,\n            e_power_is_positive=e_power_result.is_positive if e_power_result else None,\n            optimal_lambda=e_power_result.optimal_lambda if e_power_result else None\n        )\n    \n    def reset(self):\n        self._reset_state()\n        self.e_process = EProcess(config=self.config)\n\n    def get_history_df(self) -> pd.DataFrame:\n        return pd.DataFrame(self.history)",
    "uncovered_lines": [
      164,
      172,
      173,
      174,
      175,
      177,
      193
    ]
  }
]